{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion Mining & Sentiment Analysis: Deep Learning Techniques\n",
    "\n",
    "**Text Mining unit**\n",
    "\n",
    "_Prof. Gianluca Moro, Dott. Ing. Roberto Pasolini – DISI, University of Bologna_\n",
    "\n",
    "**Bologna Business School** - Alma Mater Studiorum Università di Bologna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import external libraries (thus verifying they are correctly installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to download data files if they are not already present in working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "def download(file, url):\n",
    "    if not os.path.isfile(file):\n",
    "        urlretrieve(url, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning and Neural Networks for Text Mining and Sentiment Analysis\n",
    "\n",
    "_Deep learning_ denotes a general approach to machine learning employing **multi-layered models** to obtain **accurate representation of input data**: features are no longer extracted manually, but infered in the learning process\n",
    "\n",
    "DL is mostly based on _neural networks_, very flexible learning models with arbitrary complexity\n",
    "\n",
    "The use of DL and NN allows deep understanding of text without manually defining rules, lexicons, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow and Keras\n",
    "\n",
    "- **TensorFlow** by Google is one of the most used computation frameworks for deep learning\n",
    "  - TF works by building a _computational graph_ where each node represents an operation between _tensors_ (N-dimensional arrays)\n",
    "    - sums, products, derivatives, ...\n",
    "  - a graph can run either on CPU or (where available) on GPU for accelerated parallel computation\n",
    "- **Keras** provides an high-level API for building and training neural networks using TensorFlow as a backend\n",
    "  - networks can be built simply by stacking different layers with many configurable (hyper)parameters\n",
    "  - high-level commands are provided to train and evaluate networks on given datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Movie Reviews\n",
    "\n",
    "We have a collection of user reviews extracted from IMDb (the _Internet Movie Database_) labeled as positive or negative\n",
    "\n",
    "We want to train a model to understand the positive or negative orientation of any review\n",
    "\n",
    "We start by loading the training dataset, containing 25,000 samples with two attributes\n",
    "- `label` indicates the orientation of the review, can be \"pos\" or \"neg\"\n",
    "- `text` contains the full text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"imdb-train.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-train.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"imdb-train.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view some rows of the dataset, after increasing the lenght of shown text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>If you like adult comedy cartoons, like South Park, then this is nearly a similar format about t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>Robert DeNiro plays the most unbelievably intelligent illiterate of all time. This movie is so w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this sea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   pos   \n",
       "1   neg   \n",
       "2   pos   \n",
       "3   neg   \n",
       "4   pos   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...  \n",
       "1  Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a t...  \n",
       "2  If you like adult comedy cartoons, like South Park, then this is nearly a similar format about t...  \n",
       "3  Robert DeNiro plays the most unbelievably intelligent illiterate of all time. This movie is so w...  \n",
       "4  Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this sea...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some HTML tags are present within reviews, specifically `<br />` to indicate newlines: we write a function which, applied on a text, replaces such tags with ASCII newline `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tags(text):\n",
    "    return text.replace(\"<br />\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function to all texts in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"text\"] = train_set[\"text\"].apply(strip_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive and negative reviews are evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    12500\n",
       "pos    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "In their usual form, neural networks are composed by a stack of _densely connected_ layers of nodes: each node in a layer receives the output of all nodes of the underlying layer. Such networks are also known as _multi-layer perceptrons_.\n",
    "\n",
    "A MLP receives a vector as input and its topmost layer produces a vector as output, an arbitrary number of _hidden layers_ can be inserted inbetween to produce intermediate representations of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by training a neural network for sentiment classification feeded with vector space model representations of reviews\n",
    "\n",
    "We initialize a vector space using tf.idf term weighting and filtering out very rare terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(min_df=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such vector space is built upon the training reviews and their document-term matrix is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtm = vect.fit_transform(train_set[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to training matrices for scikit-learn models, this is a 2D array where each row (1st axis) is a training observation and each column (2nd axis) is a feature: each row is a possible input to the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the number of distinct terms in the vector space, used to define the structure of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_terms = len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35852"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our neural network to indicate in output the correct class of each review, either \"pos\" or \"neg\"\n",
    "\n",
    "The common approach to classification with neural networks is to have one output node for each class and train them to output 1 on the right node and 0 on the others\n",
    "\n",
    "For this, we must extract from the `label` column a \"target\" matrix, where each row contains the values which the network should give as output for each review\n",
    "- `[1, 0]` for positive reviews\n",
    "- `[0, 1]` for negative reviews\n",
    "\n",
    "We define a function `make_target` which converts a given pos/neg labels series into a target matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(labels):\n",
    "    return pd.DataFrame({\n",
    "        \"pos\": labels == \"pos\",\n",
    "        \"neg\": labels == \"neg\"\n",
    "    }).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply it to the training set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = make_target(train_set[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a matrix where each row is the expected network output, either `[1, 0]` (positive) or `[0, 1]` (negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos  neg\n",
       "0    1    0\n",
       "1    0    1\n",
       "2    1    0\n",
       "3    0    1\n",
       "4    1    0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the structure of the neural network\n",
    "\n",
    "We create a _sequential_ model, i.e. we define the network as a sequence of interconnected layers (alternatively, we could create non-linear structures by manually connecting layers to each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example we create a single-layered network, where inputs are directly connected to the output nodes\n",
    "\n",
    "As discussed above, the output nodes must be 2, one for each class; we use the _softmax_ activation function to ensure that the output is a valid probability distribution\n",
    "- we will never get a perfect `[1, 0]` as output in practice, but we will get outputs like `[0.99, 0.01]`\n",
    "\n",
    "In the first layer (in this case the only one) we also have to specify with `input_dim` the size of input vectors, in this case the number of terms in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rrobby/software/miniconda3/envs/dltutorial/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "model.add(Dense(2, activation=\"softmax\", input_dim=num_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `summary` we can analyze the structure of our network and get the count of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 71706     \n",
      "=================================================================\n",
      "Total params: 71,706\n",
      "Trainable params: 71,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have 35,852×2 = 71,704 weights + 2 biases = 71,706 trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the network structure, we _compile_ it to provide some general settings of the network and initialize accordingly the underlying TensorFlow data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The _optimizer_ is the algorithm used to train the network: _Adam_ and other valid options are different variants of stochastic gradient descent (SGD), including learning rate decay, momentum, etc.\n",
    "- The _loss_ is the measure to be minimized in the training process: the _cross entropy_ penalizes outputs which are not close to 1 on the correct class\n",
    "- Additional _metrics_ can be computed for evaluation purposes: we use the accuracy, i.e. the percentage of correctly classified examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train (_fit_) the network on given training examples, composed of inputs (tf.idf vectors) and target outputs (`[1, 0]` for positive reviews and `[0, 1]` for negative)\n",
    "\n",
    "The training examples are shuffled and used to run SGD steps on _minibatches_ of a specified size (`batch_size`): this process is repeated for a given number of training _epochs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rrobby/software/miniconda3/envs/dltutorial/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s 99us/step - loss: 0.6565 - acc: 0.8006\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s 75us/step - loss: 0.5870 - acc: 0.8650\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.5327 - acc: 0.8776\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s 71us/step - loss: 0.4892 - acc: 0.8866\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.4534 - acc: 0.8933\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.4236 - acc: 0.8993\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.3982 - acc: 0.9060\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s 73us/step - loss: 0.3763 - acc: 0.9102\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.3571 - acc: 0.9157\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.3401 - acc: 0.9195\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit(train_dtm, train_target, batch_size=200, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process we see how loss and accuracy measured on the training set vary, their evolution can also be obtained from the \"history\" object returned by `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_history.history[\"loss\"], \"ro-\")\n",
    "plt.plot(fit_history.history[\"acc\"], \"bo-\")\n",
    "plt.legend([\"Train loss\", \"Train accuracy\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the plot how the loss progressively decreases and accuracy progressively increases through training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the raw output given by the network for a given input, we use the `predict` method: let's see for example the output for the first training review (labeled positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6024959 , 0.39750406]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_dtm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first class (positive) has higher probability\n",
    "\n",
    "We can directly get the predicted class index with `predict_classes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(train_dtm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the network on a separate test set of labeled reviews, provided in the `imdb-test.csv.gz` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"imdb-test.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"imdb-test.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>My boyfriend and I went to watch The Guardian.At first I didn't want to watch it, but I loved th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>This is a pale imitation of 'Officer and a Gentleman.' There is NO chemistry between Kutcher and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>My yardstick for measuring a movie's watch-ability is if I get squirmy. If I start shifting posi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   pos   \n",
       "1   neg   \n",
       "2   pos   \n",
       "3   neg   \n",
       "4   pos   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit ...  \n",
       "1  Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the ter...  \n",
       "2  My boyfriend and I went to watch The Guardian.At first I didn't want to watch it, but I loved th...  \n",
       "3  This is a pale imitation of 'Officer and a Gentleman.' There is NO chemistry between Kutcher and...  \n",
       "4  My yardstick for measuring a movie's watch-ability is if I get squirmy. If I start shifting posi...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this dataset we have 25,000 reviews evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    12500\n",
       "pos    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we apply the HTML strip function to reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[\"text\"] = test_set[\"text\"].apply(strip_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent the test reviews in the vector space created on training reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dtm = vect.transform(test_set[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert pos/neg labels for test examples into target vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = make_target(test_set[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing the test set, we can fed it to the neural network for evaluation using the `evaluate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 85us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39980619232177733, 0.87]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dtm, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method reports the loss (first value) and the accuracy (second value) measured on the given test set: our final goal is to maximize the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now introduce a _hidden layer_ in the network between input and output, for example a layer of 128 nodes with linear activation which receive input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=num_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of these 128 will be fed to the output layer, composed as above by 2 nodes with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of network parameters to be trained is now much higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 128)               4589184   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 4,589,442\n",
      "Trainable params: 4,589,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the network as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep compute times limited, we fit this and subsequent networks running only 3 training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 8s 306us/step - loss: 0.4134 - acc: 0.8499\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 7s 291us/step - loss: 0.1654 - acc: 0.9444\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 7s 293us/step - loss: 0.0887 - acc: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a56f4b0b8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dtm, train_target, batch_size=200, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate this new network on the same test set as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 138us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32061737238645555, 0.87064]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dtm, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the hidden layer we had a very slight improvement, despite the lower number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model more expressive, we have to introduce non-linearity in hidden layers: for example, we replicate the model above using sigmoid activation in the hidden layer\n",
    "\n",
    "We can create the model more concisely by provinding the list of layers to be stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation=\"sigmoid\", input_dim=num_terms),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 8s 305us/step - loss: 0.6447 - acc: 0.7328\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 7s 285us/step - loss: 0.4856 - acc: 0.8792\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 7s 286us/step - loss: 0.3455 - acc: 0.9050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a582f4c18>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dtm, train_target, batch_size=200, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 134us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35995052280426026, 0.87292]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dtm, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test a deep model with three non-linear hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, activation=\"sigmoid\", input_dim=num_terms),\n",
    "    Dense(64, activation=\"sigmoid\"),\n",
    "    Dense(16, activation=\"sigmoid\"),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 256)               9178368   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 9,195,890\n",
      "Trainable params: 9,195,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 13s 517us/step - loss: 0.6805 - acc: 0.5889\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 12s 493us/step - loss: 0.4331 - acc: 0.8516\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 12s 487us/step - loss: 0.1986 - acc: 0.9298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a4d7577f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dtm, train_target, batch_size=200, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 177us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27334151494503023, 0.88668]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dtm, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "A _word embedding_ model is a dictionary mapping each known word to a **N-dimensional vector**\n",
    "\n",
    "Such model is built by training a neural network on a bunch of text to predict the most likely word in a context defined by other words\n",
    "- training is unsupervised: no labeling of text is needed\n",
    "\n",
    "The resulting vector of each word somehow denotes its meaning: **semantically similar words are represented with similar vectors**. Moreover, operations between vectors can be used to **find words semantically related** to each other.\n",
    "\n",
    "Word embedding models can be used to represent text in NLP tasks, including sentiment analysis\n",
    "\n",
    "The **gensim** library provides means to represent and build word embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Word2Vec model\n",
    "\n",
    "We have a set of 5,000 movie reviews without any labeling: we can't train a sentiment classifier on them but we can train a word embedding model\n",
    "\n",
    "We read the compressed text file `imdb-unsup-5k.txt.gz`, containing one review per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"imdb-unsup-5k.txt.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-unsup-5k.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"imdb-unsup-5k.txt.gz\", \"rt\") as f:\n",
    "    we_train_set = [strip_tags(line.strip()) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to preprocess each review by splitting text into tokens\n",
    "\n",
    "gensim, the library used to train the word embedding model, provides a simple utility function for this\n",
    "- alternatively any tokenization function can be used, e.g. `nltk.word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 4 ms, total: 1.1 s\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "we_train_tokens = [simple_preprocess(text) for text in we_train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I admit, the great majority of films released before say 1933 are just not for me.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we_train_set[0][:82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admit', 'the', 'great', 'majority', 'of', 'films', 'released', 'before']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we_train_tokens[0][:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the token sequences to train the Word2Vec embedding model\n",
    "\n",
    "The most important parameter is the size of the word vectors we want to obtain\n",
    "- in the original Word2Vec paper 300 is indicated as a good value\n",
    "- here we use 50 as a tradeoff between accuracy and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvecs_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other relevant parameters are\n",
    "- the _window size_, i.e. the number of words before and after any word to consider as its context\n",
    "- the minimum appearances of a term to be included in the model\n",
    "\n",
    "We specify these options in the `Word2Vec` initializer, together with the set of token sequences used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.3 s, sys: 0 ns, total: 6.3 s\n",
      "Wall time: 6.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wv_model = gensim.models.Word2Vec(\n",
    "    we_train_tokens,\n",
    "    size=wordvecs_size,\n",
    "    window=5,\n",
    "    min_count=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Word2Vec model is now trained, we can get a reference to the word->vector mapping itself `wv` and drop the rest of the model object to free some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = wv_model.wv\n",
    "del wv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the word embedding model\n",
    "\n",
    "How many distinct terms are represented in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12070"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are these terms? `index2word` is an ordered list with more common terms coming first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'of', 'to', 'is', 'in', 'it', 'this', 'that', 'as']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.index2word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the vector of a word, e.g. \"excellent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5114084 , -0.7340342 ,  1.0234174 ,  0.22948086,  0.5712674 ,\n",
       "        0.37628934,  0.64706683, -1.487537  ,  0.28225353, -0.8491546 ,\n",
       "        0.9058341 , -0.1314533 ,  0.43188205,  0.50562024,  1.7984192 ,\n",
       "        0.38935652, -1.2059857 ,  1.6213577 , -0.663144  , -0.6638046 ,\n",
       "       -0.7816187 ,  0.32186472,  1.6609577 , -0.71841544,  0.487278  ,\n",
       "       -0.941309  , -0.09710978, -0.79371464, -0.76231325, -0.01406466,\n",
       "        2.067998  ,  0.20384946,  2.089359  ,  0.526346  ,  0.817104  ,\n",
       "        0.12727872, -1.4547936 , -0.1885227 ,  0.05824345,  0.82262087,\n",
       "        1.2286468 ,  1.108042  , -0.6305742 ,  0.27894366,  2.622213  ,\n",
       "       -0.36938676,  1.0290797 ,  0.49994272,  1.2949158 ,  1.2688118 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.word_vec(\"excellent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute and get normalized word vectors, used to compute cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.init_sims()   # compute and cache normalized vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07301285, -0.10479673,  0.14611144,  0.03276256,  0.08155882,\n",
       "        0.05372215,  0.09238056, -0.21237296,  0.04029683, -0.12123226,\n",
       "        0.12932429, -0.01876735,  0.06165902,  0.07218648,  0.25675705,\n",
       "        0.05558772, -0.17217639,  0.23147829, -0.09467586, -0.09477018,\n",
       "       -0.11159029,  0.04595204,  0.23713192, -0.10256687,  0.0695678 ,\n",
       "       -0.13438897, -0.01386419, -0.1133172 , -0.10883408, -0.00200799,\n",
       "        0.2952443 ,  0.02910322,  0.298294  ,  0.07514547,  0.11665645,\n",
       "        0.01817135, -0.20769824, -0.02691504,  0.00831531,  0.11744409,\n",
       "        0.17541167,  0.15819314, -0.09002594,  0.03982428,  0.37436858,\n",
       "       -0.05273668,  0.14691983,  0.07137591,  0.18487278,  0.18114597],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True indicates to normalize the vector\n",
    "wv.word_vec(\"excellent\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vector by itself doesn't give much information, but we can search for example which are the words with vectors most similar to this...\n",
    "\n",
    "Let's use the `cosine_similarities` function to compute similarity between this vector and all the other ones, stored in the `vector` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_to_excellent = wv.cosine_similarities(\n",
    "    wv.word_vec(\"excellent\"),\n",
    "    wv.vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain an array of cosine similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07619354,  0.28495064,  0.11302141, -0.1732213 ,  0.24119678],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities_to_excellent[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's label them with the term they refer to and sort by descending values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "excellent         1.000000\n",
       "outstanding       0.916733\n",
       "fine              0.894400\n",
       "brilliant         0.893838\n",
       "superb            0.886434\n",
       "amazing           0.865420\n",
       "cinematography    0.858674\n",
       "impressive        0.856665\n",
       "beautifully       0.856412\n",
       "stunning          0.855289\n",
       "dtype: float32"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(\n",
    "    similarities_to_excellent,\n",
    "    wv.index2word\n",
    ").sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we found **other words** other than \"excellent\" with a **strong positive connotation**!\n",
    "\n",
    "For this the model provides a `most_similar` method, which also removes the reference word from the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outstanding', 0.9167332053184509),\n",
       " ('fine', 0.8944001197814941),\n",
       " ('brilliant', 0.8938385248184204),\n",
       " ('superb', 0.8864337205886841),\n",
       " ('amazing', 0.8654204607009888),\n",
       " ('cinematography', 0.8586739897727966),\n",
       " ('impressive', 0.8566656112670898),\n",
       " ('beautifully', 0.8564115762710571),\n",
       " ('stunning', 0.8552893400192261),\n",
       " ('direction', 0.8523515462875366)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"excellent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly see what happens with a strongly negative word, e.g. \"terrible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.9518212080001831),\n",
       " ('scary', 0.911191999912262),\n",
       " ('totally', 0.9069154262542725),\n",
       " ('hilarious', 0.9064745903015137),\n",
       " ('awful', 0.8990105390548706),\n",
       " ('ok', 0.887744665145874),\n",
       " ('predictable', 0.8807995319366455),\n",
       " ('boring', 0.8789013028144836),\n",
       " ('disappointing', 0.8723584413528442),\n",
       " ('weird', 0.8680090308189392)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other strongly negative words are found!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another powerful function of word embedding models is to find words with specific syntactic and semantic relationships using vector arithmetics\n",
    "\n",
    "Consider the relationship _\"man\" is to \"woman\" as \"actor\" is to X_ where the model has to find out that X = \"actress\"\n",
    "\n",
    "Word2Vec produces vectors in such a way that _\"man\" - \"woman\" = \"actor\" - X_, so we can find X as the term whose vector is closest to _\"actor\" + \"woman\" - \"men\"_\n",
    "\n",
    "Let's produce the vector representation of X..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "composition = (wv.word_vec(\"actor\", True)\n",
    "             + wv.word_vec(\"woman\", True)\n",
    "             - wv.word_vec(\"man\", True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then find the words most similar to this composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actor          0.924466\n",
       "actress        0.883101\n",
       "role           0.849532\n",
       "performance    0.803194\n",
       "star           0.788036\n",
       "lead           0.781273\n",
       "villain        0.776565\n",
       "effeminate     0.768296\n",
       "oscar          0.762393\n",
       "talented       0.757082\n",
       "dtype: float32"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(\n",
    "    wv.cosine_similarities(composition, wv.vectors),\n",
    "    wv.index2word\n",
    ").sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case we can use `most_similar`, distinguishing words with positive and negative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.8831006288528442),\n",
       " ('role', 0.8495315909385681),\n",
       " ('performance', 0.8031936883926392),\n",
       " ('star', 0.7880364656448364),\n",
       " ('lead', 0.7812731266021729),\n",
       " ('villain', 0.7765644788742065),\n",
       " ('effeminate', 0.7682958245277405),\n",
       " ('oscar', 0.7623934745788574),\n",
       " ('talented', 0.7570821046829224),\n",
       " ('chaney', 0.7563522458076477)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\n",
    "    positive=[\"actor\", \"woman\"],\n",
    "    negative=[\"man\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to randomness in the training process, the correct answer \"actress\" might be the most similar word or very close to it, but still the confidence of the model is limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed our analysis on a pretrained GloVe (_Global Vectors_) word embedding model, whose training procedure is similar to Word2Vec\n",
    "\n",
    "We use a version trimmed down to the most common 100,000 terms of the 100d model trained on Wikipedia, available here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Arrays with words and vectors are provided in the `glove.npz` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"glove.npz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/glove.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"glove.npz\") as f:\n",
    "    glove_words = f[\"words\"]\n",
    "    glove_vectors = f[\"vectors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the vector size from the loaded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvecs_size = glove_vectors.shape[1]\n",
    "wordvecs_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the word embedding model from the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors(wordvecs_size)\n",
    "wv[glove_words.tolist()] = glove_vectors\n",
    "wv.init_sims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching on this model for the answer to _man : woman = actor : X_..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.9073609113693237),\n",
       " ('comedian', 0.6890829801559448),\n",
       " ('actresses', 0.6826434135437012),\n",
       " ('screenwriter', 0.6554961204528809),\n",
       " ('starred', 0.6533135175704956),\n",
       " ('starring', 0.6514241695404053),\n",
       " ('actors', 0.6402772068977356),\n",
       " ('dancer', 0.6378583908081055),\n",
       " ('singer', 0.6346279382705688),\n",
       " ('filmmaker', 0.6279778480529785)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\n",
    "    positive=[\"actor\", \"woman\"],\n",
    "    negative=[\"man\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the correct answer \"actress\" is more dominant on the other ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other examples with multiple pairs: finding the plural of a singular word..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mice', 0.710668683052063),\n",
       " ('rabbits', 0.681904673576355),\n",
       " ('rodents', 0.6771590709686279),\n",
       " ('rats', 0.6427716016769409),\n",
       " ('animals', 0.6243681907653809),\n",
       " ('monkeys', 0.6002902984619141),\n",
       " ('ferrets', 0.5910987854003906),\n",
       " ('mammals', 0.5888075828552246),\n",
       " ('foxes', 0.5750464200973511),\n",
       " ('raccoons', 0.5635854005813599)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\n",
    "    positive=[\"mouse\", \"dogs\", \"cats\"],\n",
    "    negative=[         \"dog\",  \"cat\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and finding the capital of a State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paris', 0.7584174275398254),\n",
       " ('cairo', 0.6146870851516724),\n",
       " ('london', 0.5959091186523438),\n",
       " ('versailles', 0.5937519669532776),\n",
       " ('vienna', 0.5896108150482178),\n",
       " ('brussels', 0.5775601863861084),\n",
       " ('petersburg', 0.5704914331436157),\n",
       " ('palace', 0.5681281089782715),\n",
       " ('sorbonne', 0.5556104183197021),\n",
       " ('strasbourg', 0.5552983283996582)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\n",
    "    positive=[\"france\", \"rome\",  \"berlin\"],\n",
    "    negative=[          \"italy\", \"germany\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method provided by the model is `doesnt_match` finding the word which is the least related to the others in a given list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rrobby/software/miniconda3/envs/dltutorial/lib/python3.6/site-packages/gensim/models/keyedvectors.py:876: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'keyboard'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match([\"cat\", \"mouse\", \"dog\", \"keyboard\", \"frog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing text with word embedding\n",
    "\n",
    "We now see how to leverage the word embedding model in a neural network for sentiment classification\n",
    "\n",
    "We start by tokenizing texts of training reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.19 s, sys: 64 ms, total: 5.26 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_tokens = [gensim.utils.simple_preprocess(text) for text in train_set[\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of tokenized review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[\"text\"][0][:34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'cartoon', 'comedy']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert these lists of text tokens into lists of indices of terms in the word embedding model, leaving out terms not present in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [\n",
    "    [wv.vocab[word].index for word in text if word in wv.vocab]\n",
    "    for text in train_tokens\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the begin of the review above is now represented with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152, 14, 7362, 2841, 20]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which translated back into words would be... (notice that the first term was removed because not in the embedding model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high', 'is', 'cartoon', 'comedy', 'it']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wv.index2word[i] for i in train_indices[0][:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first solution, we represent each review with the mean of normalized vectors of words contained in it: we obtain such vectors for all train reviews and stack them together in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_we_repr = np.vstack([wv.vectors_norm[indices].mean(0) for indices in train_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a MLP network with one hidden layer accepting such vectors in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation=\"sigmoid\", input_dim=wordvecs_size),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the input size of the network is much lower, so it is the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 13,186\n",
      "Trainable params: 13,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is much faster than before, so we can increment the epochs and reduce the batch size, thus making more SGD steps in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s 70us/step - loss: 0.6513 - acc: 0.6361\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 1s 52us/step - loss: 0.5685 - acc: 0.7256\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 1s 52us/step - loss: 0.5178 - acc: 0.7570\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 1s 52us/step - loss: 0.4918 - acc: 0.7719\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 1s 52us/step - loss: 0.4772 - acc: 0.7798\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 1s 53us/step - loss: 0.4693 - acc: 0.7808\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 1s 52us/step - loss: 0.4645 - acc: 0.7875\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.4608 - acc: 0.7864\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 1s 55us/step - loss: 0.4584 - acc: 0.7889\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.4549 - acc: 0.7902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49c78550f0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_we_repr, train_target, batch_size=20, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess test reviews as we did for training ones, thus extracting tokens and converting them to indices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = [gensim.utils.simple_preprocess(text) for text in test_set[\"text\"]]\n",
    "test_indices = [\n",
    "    [wv.vocab[word].index for word in text if word in wv.vocab]\n",
    "    for text in test_tokens\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and obtaining means of word vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_we_repr = np.vstack([wv.vectors_norm[indices].mean(0) for indices in test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the network on the test reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 19us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.45890153659820554, 0.78648]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_we_repr, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is not as good as before: with this representation we lose identity of the words in the documents other than their order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "MLPs are _feed-forward_ networks: their output at any time is only dependent from their input at the same time\n",
    "\n",
    "On the other side, if we somehow introduce **memory** inside a network, we can make its output dependent from current as well as past inputs, thus we can process **sequential data**\n",
    "\n",
    "_Recurrent_ neural networks include **cyclic connections** between nodes, making the output dependent from the state of the network at previous time steps and thus from previous inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential data\n",
    "\n",
    "While an input example for a MLP must be represented with a vector of size S, an example for a recurrent NN is represented with a **sequence of vectors**, fed to the network in T subsequent time steps (T is equal for all examples)\n",
    "\n",
    "Thus N input samples with input size S are no longer represented with a N×S array, but with a N×T×S array\n",
    "\n",
    "Leveraging the word embedding model, we represent each review with the **sequence of word vectors** for the terms contained in it\n",
    "- in this way, we consider both the identity of words (the vectors) and their order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the sequences of word indices `*_indices` extracted above\n",
    "\n",
    "We need to make all sequences of the same length (the T term above): we set a desired sequence size T, then we trim longer sequences to that size (taking the final T elements) and pad shorter sequences with null values: Keras' `pad_sequences` function does this\n",
    "- larger T values would make training much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_words = 200\n",
    "train_seq = pad_sequences(train_indices, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,    12,    20, 75360],\n",
       "       [    0,     0,     0, ...,    30,   541,  3442],\n",
       "       [    0,     0,     0, ...,   219,   191,   219],\n",
       "       ...,\n",
       "       [    0,     0,     0, ..., 29080,  1075,    48],\n",
       "       [  460,     4,    30, ...,   227,    30,  4254],\n",
       "       [    0,     0,     0, ...,  1666,    13, 13664]], dtype=int32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the matrix is the number of samples times the sequence length, i.e. N×T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "\n",
    "Let's now create a neural network which gets such sequences as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first insert an `Embedding` layer, which translates each received value into the word vector from the embedding model\n",
    "\n",
    "We need to specify the size of input and output and the word vectors to be used, taking them from the model; we also specify `trainable=False` to \"freeze\" our pretrained word vectors and exclude them from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "model.add(Embedding(\n",
    "    input_dim=len(wv.vocab),    # number of distinct vocabulary terms\n",
    "    output_dim=wordvecs_size,   # size of word vectors (S)\n",
    "    input_length=max_words,     # length of sequences (T)\n",
    "    weights=[wv.vectors],       # word vectors\n",
    "    trainable=False\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this layer is a N×T×S tensor, we feed it to a recurrent layer which receives S-sized vectors for T time steps\n",
    "\n",
    "_Gated Recurrent Units_ (GRU) are a simplified version of _Long Short-Term Memory_ (LSTM) units, which can potentially hold information in memory across many time steps; we use here a layer of 128 GRU cells\n",
    "\n",
    "_Dropout_ randomly drops (sets to zero) a given ratio of input values at each time step: it is a technique to prevent model overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "model.add(GRU(128, dropout=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While producing 128 output values at each time step, the GRU layer by default only returns the outputs at the final steps, i.e. when the whole input sequence has been fed to the network, thus the output size of this layer is N×128 (the time dimension collapses)\n",
    "\n",
    "We can now finalize the network with the output layer, which receives the output of the GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary gives a recap of shapes of data across network layers other than parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 100)          10000000  \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 128)               87936     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 10,088,194\n",
      "Trainable params: 88,194\n",
      "Non-trainable params: 10,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile the network and train it on the padded sequences of word indices\n",
    "- training of RNNs is quite slow, we again limit training to 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 34s 1ms/step - loss: 0.6063 - acc: 0.6547\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 31s 1ms/step - loss: 0.4394 - acc: 0.7986\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 32s 1ms/step - loss: 0.3776 - acc: 0.8341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4986038358>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_seq, train_target, batch_size=200, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now obtain the padded sequences also for the test reviews..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = pad_sequences(test_indices, max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and use them to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 23s 910us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34696731271743775, 0.8466]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_seq, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross domain classification\n",
    "\n",
    "We trained our network on reviews of movies and tested its ability to classify sentiment in reviews of movies\n",
    "\n",
    "Can we successfully apply our model to reviews pertaining to a different domain?\n",
    "\n",
    "The `yelp-test-10k.csv.gz` file contains 10,000 labeled user reviews about restaurants extracted from Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"yelp-test-10k.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/yelp-test-10k.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdom_set = pd.read_csv(\"yelp-test-10k.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>U can go there n check the car out. If u wanna buy 1 there? That's wrong move! If u even want a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>I have no idea why some people give bad reviews about this place. It goes to show you, you can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>Disgusting!  Had a Groupon so my daughter and I tried it out.  Very outdated and gaudy 80's sty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   pos   \n",
       "1   neg   \n",
       "2   pos   \n",
       "3   neg   \n",
       "4   pos   \n",
       "\n",
       "                                                                                                  text  \n",
       "0   My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfec...  \n",
       "1   U can go there n check the car out. If u wanna buy 1 there? That's wrong move! If u even want a...  \n",
       "2   I have no idea why some people give bad reviews about this place. It goes to show you, you can ...  \n",
       "3   Disgusting!  Had a Groupon so my daughter and I tried it out.  Very outdated and gaudy 80's sty...  \n",
       "4   Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdom_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    5000\n",
       "neg    5000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdom_set[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same preprocessing steps we applied above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdom_set[\"text\"] = xdom_set[\"text\"].apply(strip_tags)\n",
    "xdom_tokens = [gensim.utils.simple_preprocess(text) for text in xdom_set[\"text\"]]\n",
    "xdom_indices = [\n",
    "    [wv.vocab[word].index for word in text if word in wv.vocab]\n",
    "    for text in xdom_tokens\n",
    "]\n",
    "xdom_seq = pad_sequences(xdom_indices, max_words)\n",
    "xdom_target = make_target(xdom_set[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 9s 872us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3380804895401001, 0.8523]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xdom_seq, xdom_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is fairly accurate, although it was trained on reviews of a different domain\n",
    "\n",
    "Can we further improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the network\n",
    "\n",
    "In the `yelp-train-2k.csv.gz` we have a set of 2,000 labeled Yelp reviews which can be used for training\n",
    "\n",
    "We would like to make use of these in-domain reviews, without throwing away the model trained on the richer set of cross-domain reviews\n",
    "\n",
    "We can \"tune\" the trained model with an additional training run on the new set of reviews, thus making it more oriented to the new domain and still using knowledge from the other\n",
    "\n",
    "Let's load and view a summary of the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"yelp-train-2k.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/yelp-train-2k.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_set = pd.read_csv(\"yelp-train-2k.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Great local yoga studio. Had flexible hours like early morning and late night to fit any schedu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>I've been craving a good roast beef sandwich for a few days now, and finally had the chance to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Super tasty, love the cozy atmosphere, excellent and friendly service!  The naan was a bit thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>After waiting 4 days to get an appointment, Flores was a no show and didn't even bother to call.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>I have had my kitty Miller for 8 years. She has never been to any other vet.  I like this place...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   pos   \n",
       "1   neg   \n",
       "2   pos   \n",
       "3   neg   \n",
       "4   pos   \n",
       "\n",
       "                                                                                                  text  \n",
       "0   Great local yoga studio. Had flexible hours like early morning and late night to fit any schedu...  \n",
       "1   I've been craving a good roast beef sandwich for a few days now, and finally had the chance to ...  \n",
       "2   Super tasty, love the cozy atmosphere, excellent and friendly service!  The naan was a bit thin...  \n",
       "3     After waiting 4 days to get an appointment, Flores was a no show and didn't even bother to call.  \n",
       "4   I have had my kitty Miller for 8 years. She has never been to any other vet.  I like this place...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    1000\n",
       "pos    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_set[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and apply the usual preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_set[\"text\"] = tune_set[\"text\"].apply(strip_tags)\n",
    "tune_tokens = [gensim.utils.simple_preprocess(text) for text in tune_set[\"text\"]]\n",
    "tune_indices = [\n",
    "    [wv.vocab[word].index for word in text if word in wv.vocab]\n",
    "    for text in tune_tokens\n",
    "]\n",
    "tune_seq = pad_sequences(tune_indices, max_words)\n",
    "tune_target = make_target(tune_set[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the model training process on this set of reviews: the process is very fast due to the limited size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.3427 - acc: 0.8525\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.3033 - acc: 0.8735\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2647 - acc: 0.8940\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2344 - acc: 0.9085\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2100 - acc: 0.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4989d91cc0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tune_seq, tune_target, epochs=5, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now repeat the evaluation on the Yelp test set loaded before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 9s 858us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20322929101586343, 0.9178]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xdom_seq, xdom_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully boosted the model accuracy, combining even limited knowledge of the target domain with large knowledge extracted from a different domain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL tutorial",
   "language": "python",
   "name": "dltutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
